{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15437245",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15437245",
    "outputId": "7b77dec0-5c28-4e06-edbf-a6c14d85de85",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11946/495445053.py:5: DtypeWarning: Columns (0,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"./../_datasets/twitter_za/RSA_tweet_data.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 67585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    RT @TygressAndy: Her killer\\xe2\\x80\\x99s famil...\n",
       "1    my misandry doesn't go unjustified. \\n#menaret...\n",
       "2    RT @zozitunzi: My little sister's friend, a be...\n",
       "3    RT @MatlhagaKebo: \\xe2\\x80\\x9cWhy don\\xe2\\x80\\...\n",
       "4    RT @ElihleGwala: My heart bleeds for Kwasa\\xe2...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataload\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./../_datasets/twitter_za/RSA_tweet_data.csv\")\n",
    "df = df[\"tweet_text\"]\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i][2:len(df.iloc[i])-1]    \n",
    "    df.iloc[i] = tweet\n",
    "\n",
    "print(f\"Dataset Length: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81f5d16",
   "metadata": {
    "id": "d81f5d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 17741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def cleanText(t):\n",
    "    t = re.sub(r'http\\S+', '', t)\n",
    "    t = re.sub(r\"\\[A-Za-z]*\\.com\", \" \", t)\n",
    "    return t.replace(\".\", \" . \").replace(\",\", \" , \").replace(\";\", \" ; \").replace(\"?\", \" ? \").replace(\"!\", \" ! \").replace(\"(\", \" (\").replace(\")\", \") \").replace(\"...\", \" ... \").replace(\"\\\"\", \" \\\"\")\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_length=100):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        \n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        \n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        \n",
    "        print(f\"Vocab Size: {len(self.uniq_words)}\\n\")\n",
    "        \n",
    "    def load_words(self):\n",
    "        text = df.str.cat(sep=\" \").lower()\n",
    "        text = cleanText(text)\n",
    "        text = text.split(\" \")\n",
    "        text = [x.encode(\"ascii\", \"ignore\") for x in text]\n",
    "        text = [x.decode() for x in text if not \"\\\\x\" in x.decode()]\n",
    "        text = [x for x in text if not x == \"\"]\n",
    "        text = [x for x in text if not \"#\" in x and not \"@\" in x and not \"http\" in x and \"\\\\n\" not in x and not x == \"rt\"]\n",
    "        return text\n",
    "    \n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )\n",
    "    \n",
    "dataset = Dataset(sequence_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8e74ea",
   "metadata": {
    "id": "bf8e74ea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 5\n",
    "        \n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, self.lstm_size*2)\n",
    "        self.fc2 = nn.Linear(self.lstm_size*2, self.lstm_size)\n",
    "        self.fc3 = nn.Linear(self.lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        output = self.fc(output)\n",
    "        output = self.fc2(output)\n",
    "        logits = self.fc3(output)\n",
    "        return logits, state\n",
    "    \n",
    "    def init_state(self, sequence_length):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "            torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab941be8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab941be8",
    "outputId": "009f7b0d-233c-4cee-da23-ffb179a17292",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 17741\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2qa5or3t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>███▇▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>6.37298</td></tr><tr><td>Predictions</td><td>what does he'd revea...</td></tr><tr><td>epoch</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">5l, fc3, seq100, lr 1e-3 more clean</strong>: <a href=\"https://wandb.ai/parabyl/ZA%20Twitter%20LSTM/runs/2qa5or3t\" target=\"_blank\">https://wandb.ai/parabyl/ZA%20Twitter%20LSTM/runs/2qa5or3t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220624_004940-2qa5or3t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2qa5or3t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.19 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/c/Desktop/dev/gan-tutorial-series/fun/wandb/run-20220624_010802-qjrv7oag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/parabyl/ZA%20Twitter%20LSTM/runs/qjrv7oag\" target=\"_blank\">5l, fc3, seq100, lr 1e-3 more clean</a></strong> to <a href=\"https://wandb.ai/parabyl/ZA%20Twitter%20LSTM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 0 | l: 9.786608695983887 | b: 0/17313\n",
      "what does mjita fat peers blue actual priorities perpetrators dudes 6:00 affirmative book accord sopie mum facibg grave dressing wonderkids \"our mee\n",
      "e: 0 | l: 6.590362071990967 | b: 25/17313\n",
      "what does berry one puntshununu things and are as i blm sport carrying rt like innocent with out my ? in nasty\n",
      "e: 0 | l: 6.366405010223389 | b: 50/17313\n",
      "what does dodgy themselves: mosadi rt rt rt must , the a jack should with a call happening easy at rt cackling\n",
      "e: 0 | l: 6.561718940734863 | b: 75/17313\n",
      "what does 20x28inches showing letterhead performed go the , a here billion . ! rt sense have our a that 7 me\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# prompts = [\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#     \"lasizwe is a\",\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#     \"i am\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#     print(\" \".join(autocomplete))\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#     print()\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, num_epochs, sequence_length, batch_size, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m state_h \u001b[38;5;241m=\u001b[39m state_h\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     63\u001b[0m state_c \u001b[38;5;241m=\u001b[39m state_c\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 65\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch,\n\u001b[1;32m     71\u001b[0m })\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "# Main\n",
    "\n",
    "# Best Settings so Far\n",
    "# Model:\n",
    "# 5 LSTM Layers, embed_size and lstm_size of 128\n",
    "# 3 Linear FC Layers lstm_size => *2 => *1 => n__vocab\n",
    "# \n",
    "# Hparams: \n",
    "# SEQ_LEN 100\n",
    "# Batch 64 - increasing doesn't really help\n",
    "# Adam LR 1e-3\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 25\n",
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "LOAD_MODEL=False\n",
    "LOAD_CHECKPOINT=\"twitter_lstm_10e.pth\"\n",
    "\n",
    "dataset = Dataset(sequence_length=SEQ_LEN)\n",
    "model = Model(dataset)\n",
    "model.to(DEVICE)\n",
    "\n",
    "def train(dataset, model, num_epochs=25, sequence_length=100, batch_size=64, device=\"cpu\"):\n",
    "    print(\"Training\")\n",
    "    wandb.init(entity=\"parabyl\", project=\"ZA Twitter LSTM\", name=f\"5l, fc3, seq100, lr 1e-3 more clean\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    if LOAD_MODEL == True:\n",
    "        checkpoint=torch.load(LOAD_CHECKPOINT, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        last_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Model Loaded\")\n",
    "        print(last_epoch)\n",
    "        print(last_loss)\n",
    "        \n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "        state_h, state_c = state_h.to(device), state_c.to(device)\n",
    "        \n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1,2), y)\n",
    "            \n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            wandb.log({\n",
    "                \"Loss\": loss.item(),\n",
    "                \"epoch\": epoch,\n",
    "            })\n",
    "            \n",
    "            if batch % 25 == 0:\n",
    "                print(F\"e: {epoch} | l: {loss.item()} | b: {batch}/{len(dataloader)}\")\n",
    "                text = \" \".join(predict(dataset, model, \"what does\", device))\n",
    "                print(text)\n",
    "                wandb.log({\n",
    "                    \"Predictions\": text,\n",
    "                })\n",
    "                model.train()\n",
    "                \n",
    "            \n",
    "            if batch % 100 == 0: \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'loss': loss, \n",
    "                }, \"twitter_lstm_10plus.pth\")\n",
    "        \n",
    "        \n",
    "def predict(dataset, model, text, device, next_words=20):\n",
    "    if LOAD_MODEL == True:\n",
    "        print(\"Loading model for Prediction\")\n",
    "        checkpoint=torch.load(LOAD_CHECKPOINT, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "    \n",
    "    words = text.split(\" \")\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "    state_h, state_c = state_h.to(device), state_c.to(device)\n",
    "    \n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        \n",
    "        last_word_logits = y_pred[0][-1].detach()\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).cpu().detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "        \n",
    "    return words\n",
    "\n",
    "# prompts = [\n",
    "#     \"lasizwe is a\",\n",
    "#     \"i am\"\n",
    "# ]\n",
    "\n",
    "# for p in range(len(prompts)):\n",
    "#     prompt = prompts[p]\n",
    "#     autocomplete = predict(dataset, model, prompt, device=DEVICE, next_words=100-len(prompt))\n",
    "#     print(\" \".join(autocomplete))\n",
    "#     print()\n",
    "    \n",
    "train(dataset, model, num_epochs=NUM_EPOCHS, sequence_length=SEQ_LEN, batch_size=BATCH_SIZE, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ZA_Twitter_LSTM.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c7a21014fc0903b333c528e26b532495acabffc408f92f7990944da68b6f70a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
